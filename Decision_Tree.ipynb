{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CART算法实现"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# 创建测试数据集\n",
    "def createDataSet():\n",
    "    dataSet = [[0, 0, 0, 0, 'no'],  # 数据集\n",
    "               [0, 0, 0, 1, 'no'],\n",
    "               [0, 1, 0, 1, 'yes'],\n",
    "               [0, 1, 1, 0, 'yes'],\n",
    "               [0, 0, 0, 0, 'no'],\n",
    "               [1, 0, 0, 0, 'no'],\n",
    "               [1, 0, 0, 1, 'no'],\n",
    "               [1, 1, 1, 1, 'yes'],\n",
    "               [1, 0, 1, 2, 'yes'],\n",
    "               [1, 0, 1, 2, 'yes'],\n",
    "               [2, 0, 1, 2, 'yes'],\n",
    "               [2, 0, 1, 1, 'yes'],\n",
    "               [2, 1, 0, 1, 'yes'],\n",
    "               [2, 1, 0, 2, 'yes'],\n",
    "               [2, 0, 0, 0, 'no']]\n",
    "    labels = ['年龄', '有工作', '有自己的房子', '信贷情况']\n",
    "    return dataSet, labels"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# 计算经验熵\n",
    "def cal_empirical_entropy(data_vector):\n",
    "    nums_data=len(data_vector)   #数据集样本数\n",
    "    counts_by_labels={}      #用来保存每个label下的样本数\n",
    "    entroy=0\n",
    "    for vector in data_vector:\n",
    "        if vector[-1] not in counts_by_labels:       #vector[-1]为label值\n",
    "            counts_by_labels[vector[-1]]=0\n",
    "        counts_by_labels[vector[-1]]+=1              #统计label出现的次数\n",
    "    for key in counts_by_labels:\n",
    "        p=float(counts_by_labels[key]/nums_data)          #计算每个标签出现的概率\n",
    "        entroy-=p*math.log(p,2)                     #计算经验熵，公式5.7\n",
    "    return entroy\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# 划分数据集\n",
    "# index_feature 特征的索引位置\n",
    "# value 划分特征的取值\n",
    "def split_datatset(data_vector,index_feature,value):\n",
    "    split_set=[]\n",
    "    for vector in data_vector:\n",
    "        # 挑选vector[index_feature]==value的数据\n",
    "        if vector[index_feature]==value:\n",
    "            # 去掉第i列特征\n",
    "            split_1=vector[:index_feature]\n",
    "            split_1.extend(vector[index_feature+1:])\n",
    "            split_set.append(split_1)\n",
    "    return len(split_set), split_set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# 返回较大一方所对应位置的值和索引\n",
    "def choose_max(fea_x1,max_x2,fea_index1,max_indx2):\n",
    "    if fea_x1>max_x2:\n",
    "        return fea_x1,fea_index1\n",
    "    else:\n",
    "        return max_x2,max_indx2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# 选择最优分类特征\n",
    "def choose_bestfeture(data_vector,create_alg_para):\n",
    "    nums_data=len(data_vector) # 数据数量\n",
    "    nums_feature=len(data_vector[0])-1       #每个样本所包含的特征个数\n",
    "    empirical_entropy=cal_empirical_entropy(data_vector)   #计算经验熵\n",
    "    max_information_gain=0                     #表示最大信息增益\n",
    "    max_information_gain_ratio=0                 #表示最大的信息增益比\n",
    "    best_index_feature=0                         #表示最优特征的索引位置index\n",
    "    for i in range(nums_feature): # 遍历所有特征\n",
    "        features_i_set=[vector[i] for vector in data_vector]           #提取第i个特征中所包含的可能取值\n",
    "        features_i_set=set(features_i_set)             #对特征值去重\n",
    "        conditional_entroy=0                        #表示每个特征的经验条件熵，公式5.8\n",
    "        ha_d_entroy=0                               #表示数据集D关于特征A的值的熵Ha(D)，公式5.10\n",
    "        for fea in features_i_set:\n",
    "            nums_di,di_set=split_datatset(data_vector,i,fea) # 得到划分的特征与相应数据\n",
    "            p_di=nums_di/nums_data                 #计算|Di|/|D|，公式5.8\n",
    "            ha_d_entroy-=p_di*math.log(p_di,2)     #计算数据集D关于特征A的值的熵Ha(D)，参考公式5.10\n",
    "            entroy_di=cal_empirical_entropy(di_set)   #计算子类的经验熵，公式5.8中的H(Di)\n",
    "        fea_information_gain=empirical_entropy-conditional_entroy    #计算每个特征对应的信息增益，公式5.9\n",
    "        fea_information_gain_ratio = fea_information_gain/ha_d_entroy # 信息增益比\n",
    "\n",
    "        # 通过create_alg_para参数选择是ID3还是C4.5算法。\n",
    "        if create_alg_para == \"ID3\":\n",
    "            max_information_gain,best_index_feature=choose_max(fea_information_gain,max_information_gain,i,best_index_feature)\n",
    "        elif create_alg_para == \"C45\":\n",
    "            max_information_gain_ratio,best_index_feature=choose_max(fea_information_gain_ratio,max_information_gain_ratio,i,best_index_feature)\n",
    "        else:\n",
    "            exit(\"create_alg_para should be 'ID3' or 'C45'.\")\n",
    "\n",
    "    return best_index_feature\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# 返回类列表中出现次数最多的类标签\n",
    "def max_class(label_list):\n",
    "    count_label={}\n",
    "    for label in label_list:\n",
    "        if label not in count_label:\n",
    "            count_label[label]=0\n",
    "        count_label[label]+=1\n",
    "    #     选择字典value最大的所对应的key值\n",
    "    return max(count_label,key=count_label.get)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'有工作': {0: {'有自己的房子': {0: 'no', 1: 'yes'}}, 1: 'yes'}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class Decision_tree(object):\n",
    "    def __init__(self,data_vector,labels,create_alg_para='C45'):\n",
    "        # 数据集\n",
    "        self.data_vector=data_vector\n",
    "        # 特征标签\n",
    "        self.labels=labels\n",
    "        # 生成决策树的方法：ID3或者C45\n",
    "        self.create_alg_para=create_alg_para\n",
    "\n",
    "    # 生成决策树，返回决策树tree，字典形式\n",
    "    def tree_main(self):\n",
    "        tree=self.create_decision_tree(self.data_vector,self.labels)\n",
    "        return tree\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    递归函数，用于生成每一个子树,并返回。\n",
    "    《统计学习方法》ID3或C4.5算法\n",
    "    data_vector：每一个待分类数据集\n",
    "    labels：待分类特征标签\n",
    "\n",
    "    \"\"\"\n",
    "    def create_decision_tree(self,data_vector,labels):\n",
    "        nums_label=[vector[-1] for vector in data_vector]\n",
    "        # 如果数据集中所有实例属于同一个类，则停止划分。返回该类 标签。\n",
    "        if len(set(nums_label))==1:\n",
    "            return nums_label[0]\n",
    "        # print(\"a\",'\\n',data_vector)\n",
    "        # 如果特征集只有一类时，即已经遍历完了所有特征，则停止划分。返回出现次数最多的类标签\n",
    "        if len(data_vector[0])==1:\n",
    "            return max_class(nums_label)\n",
    "\n",
    "        best_index_feature=choose_bestfeture(data_vector,self.create_alg_para)    #选择最优特征\n",
    "        best_feature_label=labels[best_index_feature]         #最优特征的标签\n",
    "        myTree = {best_feature_label: {}}                    #子决策树，key为最优特征的标签，value为子决策树\n",
    "        del (labels[best_index_feature])                    #删除已经使用过的最优特征标签\n",
    "        best_feature_value = [vector[best_index_feature] for vector in data_vector]\n",
    "        best_feature_set = set(best_feature_value )\n",
    "        # 根据最优特征标签的属性值划分新的子数据集，并递归生成子树\n",
    "        for f_value in best_feature_set:\n",
    "            nums_data_vector,data_vector_split=split_datatset(data_vector,best_index_feature,f_value)\n",
    "            myTree[best_feature_label][f_value]=self.create_decision_tree(data_vector_split,labels)\n",
    "        return  myTree\n",
    "\n",
    "    def cart(self):\n",
    "        # CART算法参考下一篇博客\n",
    "        pass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataSet, labels = createDataSet()\n",
    "    # best=choose_bestfeture(dataSet)\n",
    "    # print(best)\n",
    "\n",
    "    # create_alg_para should be 'ID3' or 'C45'\n",
    "    tree=Decision_tree(dataSet,labels,create_alg_para=\"C45\")\n",
    "    print(tree.tree_main())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
